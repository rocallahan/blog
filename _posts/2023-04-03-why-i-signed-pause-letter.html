---
layout: "post"
title: "Why I Signed The \"Pause\" Letter"
date: "2023-04-03 10:39:00 +0000"
permalink: "/2023/04/why-i-signed-pause-letter.html"
---
<p><em>I work for Google, but everything here is my personal opinion, not Google's position.</em>
<p>I signed <a href="https://futureoflife.org/open-letter/pause-giant-ai-experiments">the "Pause" letter</a> because <strong>I agree with what it says and I think the actions it advocates are very likely to be beneficial</strong>.
<p>To be clear, none of the following are reasons why I signed:
<ul>
<li>Because I'm a fan of the Future Of Life Institute, Elon Musk, or any of the other signatories
<li>Because I think the recommended pause would solve all our problems
<li>Because I'm confident the pause will actually happen
<li>To give academics <a href="https://scottaaronson.blog/?p=7174">time to publish papers</a> (Scott Aaronson, this one's beneath you)
<li>To build hype around large language models
<li>To give some company an advantage over another company
</ul>
<hr>
<p>Most of the arguments against a pause are very shallow, e.g. LLMs are a hoax; the proposed pause in giant-model training could lead to government regulation which is inevitably worse than any alternative; people want a pause because of risk X when they should be worried about worse risk Y; AI is so obviously super-beneficial that it's morally wrong to delay those benefits. The two strongest arguments against a pause, IMHO, are a) China and b) potential overhang.
<p>Some people argue that a pause would give China a chance to overtake the rest of the world in AI, and that would be worse than not having a pause. I think that is very unlikely. Western-aligned countries together seem to have three big advantages: a lead in training large models, control over the manufacture and distribution of the most powerful GPUs and TPUs, and most of the world's best AI researchers. Also, from what I've read, the modern CCP is highly averse to social disruption, so I expect them to proceed carefully.
<p>Another argument against a pause is that if we stop training giant models we'll continue to improve hardware and algorithms, so the next time we train a giant model there would be a discontinuous increase in capability, which would be worse than not pausing at all. It seems unlikely to me that this effect, even if it happens, would outweight the benefits of a pause.
<hr>
<p>Supporters of the proposed pause are motivated by different AI risks. In particular:
<ul>
<li>Some people, <a href="https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/">exemplified by Eliezer Yudkowsky</a>, are convinced AI will inevitably destroy humanity due to the "alignment problem". I think that possibility has to be taken seriously &mdash; which alone is a good reason to adopt the "Pause" letter's measures (and more). Still, I doubt doom is guaranteed.
<li>Some people are more worried about socioeconomic issues like potential mass unemployment. I think these issues, taken together, are also a sufficient reason to slow down AI development and mass deployment. When AI capabilities grow faster than people can learn new skills, we're pushing the limit of the rate of change humans can handle.
<li>In between "social/economic adjustment" and "alignment extinction risk" are the risks of powerful AIs in the hands of malicious people. We have to learn to defend against the new generations of scams, hacks, warfare and other havoc that are coming, and again, this takes time.
</ul>
<p>I think <em>all</em> of these risks are valid concerns that warrant great caution and slower rates of change. I'd be delighted if we got the proposed pause. I don't think we will, but I hope that these issues get a lot more attention and we start taking government and self-regulation of AI seriously. It's absurd that we regulate food and drug safety but not AI.
<div class='comments'><h2>Comments</h2>
<div class='comment'>
<div class='author'>Josh Cogliati</div>
<div class='content'>I also signed the pause letter. I think that there is no hope that we will be able to control superintelligent AIs, but I do think there is a chance that we might end up with an ethical AI. (ChatGPT definitely has a much better understanding of Human ethics than I could write a program to have, but just because a being knows what good is doesn&#39;t mean that being will act good.)</div>
</div>
<div class='comment'>
<div class='author'>Data Science</div>
<div class='content'> &gt;&gt;𝑊ℎ𝑒𝑛 𝐴𝐼 𝑐𝑎𝑝𝑎𝑏𝑖𝑙𝑖𝑡𝑖𝑒𝑠 𝑔𝑟𝑜𝑤 𝑓𝑎𝑠𝑡𝑒𝑟 𝑡ℎ𝑎𝑛 𝑝𝑒𝑜𝑝𝑙𝑒 𝑐𝑎𝑛 𝑙𝑒𝑎𝑟𝑛 𝑛𝑒𝑤 𝑠𝑘𝑖𝑙𝑙𝑠, 𝑤𝑒&#39;𝑟𝑒 𝑝𝑢𝑠ℎ𝑖𝑛𝑔 𝑡ℎ𝑒 𝑙𝑖𝑚𝑖𝑡 𝑜𝑓 𝑡ℎ𝑒 𝑟𝑎𝑡𝑒 𝑜𝑓 𝑐ℎ𝑎𝑛𝑔𝑒 ℎ𝑢𝑚𝑎𝑛𝑠 𝑐𝑎𝑛 ℎ𝑎𝑛𝑑𝑙𝑒
Rob,
What&#39;s your take on GPT 3.5, regarding any impact on NZ&#39;s Software Engineering / Data Science job markets over the coming year or so?</div>
</div>

</div>